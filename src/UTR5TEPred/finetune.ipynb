{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d686c1-578a-4687-8573-1a73c02d08ce",
   "metadata": {},
   "source": [
    "# Fine-tune LAMAR to predict translation efficiencies of mRNAs based on 5' UTRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37713950-cd67-47b4-9c25-ae4fb2d5f2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LAMAR.sequence_classification_patch import EsmForSequenceClassification\n",
    "from transformers import AutoConfig, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, load_model\n",
    "import evaluate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8410c91-3018-4bd3-bf16-61a0d957c897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('/picb/rnasys2/zhouhanwen/github/LAMAR/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c361066c-8ae0-4a73-9839-766a7137a500",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "**The following parameters can be changed.**    \n",
    "nlabels: num of predicted labels, = 1 means regression, = 2 means binary classification    \n",
    "data_path: path of finetuning data  \n",
    "pretrain_state_path: path of pretraining weights  \n",
    "batch_size: <= 16 for single card, here we use V100 32G  \n",
    "peak_lr: peak learning rate, 1e-5 ~ 1e-4 in most conditions   \n",
    "total_epochs: num of finetuning epochs  \n",
    "accum_steps: accumulation steps if using gradient accumulation  \n",
    "output_dir: path of saving model  \n",
    "logging_steps: num of training steps to log loss value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa63d2-3e82-42c5-93d6-7cf3aa84c069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_path = 'tokenizer/single_nucleotide/'\n",
    "model_max_length = 1026\n",
    "model_name = 'config/config_150M.json'\n",
    "token_dropout = False\n",
    "positional_embedding_type = 'rotary'\n",
    "hidden_size = 768\n",
    "intermediate_size = 3072\n",
    "num_attention_heads = 12\n",
    "num_hidden_layers = 12\n",
    "nlabels = 1\n",
    "data_path = 'UTR5TEPred/data/te_single_nucleotide_5/'\n",
    "pretrain_state_path = 'pretrain/saving_model/mammalian80D_2048len1mer1sw_80M/checkpoint-250000/model.safetensors'\n",
    "# pretrain_state_path = None\n",
    "batch_size = 16\n",
    "peak_lr = 5e-5\n",
    "warmup_ratio = 0.05\n",
    "total_epochs = 32\n",
    "grad_clipping_norm = 1\n",
    "accum_steps = 1\n",
    "output_dir = 'UTR5TEPred/saving_model/mammalian_2048/bs16_lr5e-5_wr0.05_32epochs_5'\n",
    "save_epochs = 100\n",
    "logging_steps = 100\n",
    "fp16 = False\n",
    "flash_attention = False\n",
    "head_type = 'Linear'\n",
    "freeze = False\n",
    "kernel_sizes = [2, 3, 5]\n",
    "ocs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd7fe9-8c45-4094-8806-35fab507d758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "# We found that padding_side=\"left\" can achieve better results when predicting translation efficiency based on 5' UTRs\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, model_max_length=model_max_length, padding_side='left')\n",
    "\n",
    "# Config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, vocab_size=len(tokenizer), pad_token_id=tokenizer.pad_token_id, mask_token_id=tokenizer.mask_token_id, num_labels=nlabels, \n",
    "    token_dropout=token_dropout, positional_embedding_type=positional_embedding_type, \n",
    "    hidden_size=hidden_size, intermediate_size=intermediate_size, num_attention_heads=num_attention_heads, num_hidden_layers=num_hidden_layers\n",
    ")\n",
    "\n",
    "# Training data\n",
    "data = load_from_disk(data_path)\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da207840-3deb-44df-a605-019e96726ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = EsmForSequenceClassification(config, head_type=head_type, freeze=freeze, kernel_sizes=kernel_sizes, ocs=ocs)\n",
    "if flash_attention:\n",
    "    from flash_attn_patch import EsmSelfAttentionAddFlashAttnPatch\n",
    "    for i in range(config.num_hidden_layers):\n",
    "        model.esm.encoder.layer[i].attention.self = EsmSelfAttentionAddFlashAttnPatch(config, position_embedding_type='rotary')\n",
    "if pretrain_state_path:\n",
    "    print(\"Loading parameters of pretraining model: {}\".format(pretrain_state_path))\n",
    "    if pretrain_state_path.endswith('.bin'):\n",
    "        model.load_state_dict(torch.load(pretrain_state_path), strict=False)\n",
    "    elif pretrain_state_path.endswith('.safetensors'):\n",
    "        load_model(model, filename=pretrain_state_path, strict=False)\n",
    "else:\n",
    "    print(\"No Loading parameters of pretraining model !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f3f05-b24f-4ac2-8fe7-2e963bd4781d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "train_args = TrainingArguments(\n",
    "    disable_tqdm=False, \n",
    "    save_total_limit=1, \n",
    "    dataloader_drop_last=True, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=1, \n",
    "    learning_rate=peak_lr, \n",
    "    weight_decay=0.01, \n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.98, \n",
    "    adam_epsilon=1e-8, \n",
    "    warmup_ratio=warmup_ratio, \n",
    "    num_train_epochs=total_epochs, \n",
    "    max_grad_norm=grad_clipping_norm, \n",
    "    gradient_accumulation_steps=accum_steps, \n",
    "    output_dir=output_dir, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=logging_steps, \n",
    "    save_strategy='steps', \n",
    "    save_steps=save_epochs, \n",
    "    logging_strategy = 'steps', \n",
    "    logging_steps=logging_steps, \n",
    "    fp16=fp16, \n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827648c2-a7ea-41c5-8521-4670a6cb60f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    labels: true labels\n",
    "    predictions: predict labels\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.squeeze()\n",
    "    mse = np.mean((predictions - labels) ** 2)\n",
    "    df = pd.DataFrame({'pred': predictions, 'label': labels})\n",
    "    corr_coef_pearson = df.corr(method='pearson').iloc[0, 1]\n",
    "    corr_coef_spearman = df.corr(method='spearman').iloc[0, 1]\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"corr_coef_pearson\": corr_coef_pearson, \n",
    "        \"corr_coef_spearman\": corr_coef_spearman\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b169065-db70-40f9-a38e-90f0cb8f7e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=data['train'], \n",
    "    eval_dataset=data['test'], \n",
    "    compute_metrics=compute_metrics, \n",
    "    data_collator=data_collator, \n",
    "    tokenizer=tokenizer\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c0339-4f77-40b2-a220-6daba08c6630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba67ac-fe13-4b4c-8090-a0023079dca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch201",
   "language": "python",
   "name": "torch201"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
