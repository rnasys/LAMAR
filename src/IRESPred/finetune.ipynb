{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa9717c-20ea-4be4-b32b-e3d0f47eab9b",
   "metadata": {},
   "source": [
    "# Fine-tune LAMAR to predict IRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072b96ba-bb87-450c-b1b7-78617233a3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch201/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from LAMAR.sequence_classification_patch import EsmForSequenceClassification\n",
    "from transformers import AutoConfig, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, load_model\n",
    "import evaluate\n",
    "from sklearn.metrics import precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b849cd-3967-43ed-8809-b1fe43011c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('/picb/rnasys2/zhouhanwen/github/LAMAR/')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2731d3-d8ed-468c-9eb5-ad1eda5bbd7a",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "**The following parameters can be changed.**    \n",
    "nlabels: num of predicted labels, = 1 means regression, = 2 means binary classification    \n",
    "data_path: path of finetuning data  \n",
    "pretrain_state_path: path of pretraining weights  \n",
    "batch_size: <= 8 for single card, here we use V100 32G  \n",
    "peak_lr: peak learning rate, 1e-5 ~ 1e-4 in most conditions   \n",
    "total_epochs: num of finetuning epochs  \n",
    "accum_steps: accumulation steps if using gradient accumulation  \n",
    "output_dir: path of saving model  \n",
    "logging_steps: num of training steps to log loss value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9f6a19-e9d0-4bb3-88cc-4ef98494704d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_path = 'tokenizer/single_nucleotide/'\n",
    "model_max_length = 1500\n",
    "model_name = 'config/config_150M.json'\n",
    "token_dropout = False\n",
    "positional_embedding_type = 'rotary'\n",
    "hidden_size = 768\n",
    "intermediate_size = 3072\n",
    "num_attention_heads = 12\n",
    "num_hidden_layers = 12\n",
    "nlabels = 2\n",
    "data_path = 'IRESPred/data/IRES_4/'\n",
    "pretrain_state_path = 'pretrain/saving_model/mammalian80D_2048len1mer1sw_80M/checkpoint-250000/model.safetensors'\n",
    "# pretrain_state_path = None\n",
    "batch_size = 8\n",
    "peak_lr = 1e-4\n",
    "warmup_ratio = 0.05\n",
    "total_epochs = 4\n",
    "grad_clipping_norm = 1\n",
    "accum_steps = 2\n",
    "output_dir = 'IRESPred/saving_model/mammalian_2048/bs16_lr1e-4_wr0.05_4epochs_4'\n",
    "save_epochs = 10\n",
    "logging_steps = 100\n",
    "fp16 = False\n",
    "flash_attention = False\n",
    "head_type = 'Linear'\n",
    "freeze = False\n",
    "kernel_sizes = [2, 3, 5]\n",
    "ocs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87463c46-ca22-48fe-bd04-31ff70db3dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, model_max_length=model_max_length)\n",
    "\n",
    "# Config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, vocab_size=len(tokenizer), pad_token_id=tokenizer.pad_token_id, mask_token_id=tokenizer.mask_token_id, num_labels=nlabels, \n",
    "    token_dropout=token_dropout, positional_embedding_type=positional_embedding_type, \n",
    "    hidden_size=hidden_size, intermediate_size=intermediate_size, num_attention_heads=num_attention_heads, num_hidden_layers=num_hidden_layers\n",
    ")\n",
    "\n",
    "# Training data\n",
    "data = load_from_disk(data_path)\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b54d20-5715-4c47-b1d2-d47a7ca29233",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parameters of pretraining model: pretrain/saving_model/mammalian80D_2048len1mer1sw_80M/checkpoint-250000/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = EsmForSequenceClassification(config, head_type=head_type, freeze=freeze, kernel_sizes=kernel_sizes, ocs=ocs)\n",
    "if flash_attention:\n",
    "    from flash_attn_patch import EsmSelfAttentionAddFlashAttnPatch\n",
    "    for i in range(config.num_hidden_layers):\n",
    "        model.esm.encoder.layer[i].attention.self = EsmSelfAttentionAddFlashAttnPatch(config, position_embedding_type='rotary')\n",
    "if pretrain_state_path:\n",
    "    print(\"Loading parameters of pretraining model: {}\".format(pretrain_state_path))\n",
    "    if pretrain_state_path.endswith('.pt'):\n",
    "        model.load_state_dict(torch.load(pretrain_state_path)['MODEL_STATE'], strict=False)\n",
    "    elif pretrain_state_path.endswith('.bin'):\n",
    "        model.load_state_dict(torch.load(pretrain_state_path), strict=False)\n",
    "    elif pretrain_state_path.endswith('.safetensors'):\n",
    "        load_model(model, filename=pretrain_state_path, strict=False)\n",
    "else:\n",
    "    print(\"No Loading parameters of pretraining model !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2004b4c4-fead-4c58-9f0d-2af5bdae16da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "train_args = TrainingArguments(\n",
    "    disable_tqdm=False, \n",
    "    save_total_limit=1, \n",
    "    dataloader_drop_last=True, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    learning_rate=peak_lr, \n",
    "    weight_decay=0.01, \n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.98, \n",
    "    adam_epsilon=1e-8, \n",
    "    warmup_ratio=warmup_ratio, \n",
    "    num_train_epochs=total_epochs, \n",
    "    max_grad_norm=grad_clipping_norm, \n",
    "    gradient_accumulation_steps=accum_steps, \n",
    "    output_dir=output_dir, \n",
    "    save_strategy='steps', \n",
    "    save_steps=save_epochs, \n",
    "    logging_strategy = 'steps', \n",
    "    logging_steps=logging_steps, \n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=logging_steps, \n",
    "    fp16=fp16, \n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef1701e-2f48-44f4-85a4-ab6a722625af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    labels: true labels\n",
    "    predictions: predict labels\n",
    "    pred_probs: predict probabilities\n",
    "    \"\"\"\n",
    "    # metrics\n",
    "    accuracy = evaluate.load(\"metrics/accuracy\")\n",
    "    precision = evaluate.load(\"metrics/precision\")\n",
    "    recall = evaluate.load(\"metrics/recall\")\n",
    "    f1 = evaluate.load(\"metrics/f1\")\n",
    "    roc_auc = evaluate.load(\"metrics/roc_auc\")\n",
    "    \n",
    "    predictions, labels = p\n",
    "    pred_probs = np.exp(predictions) / np.sum(np.exp(predictions), axis=1, keepdims=True)\n",
    "    predictions = np.argmax(predictions, axis=1).flatten()\n",
    "    labels = np.array(labels).flatten()\n",
    "    \n",
    "    accuracy_v = accuracy.compute(references=labels, predictions=predictions)\n",
    "    precision_v = precision.compute(references=labels, predictions=predictions, zero_division=0)\n",
    "    recall_v = recall.compute(references=labels, predictions=predictions)\n",
    "    f1_v = f1.compute(references=labels, predictions=predictions)\n",
    "    roc_auc_v = roc_auc.compute(references=labels, prediction_scores=pred_probs[:, 1])\n",
    "    precision_prauc, recall_prauc, threshold_prauc = precision_recall_curve(labels, pred_probs[:, 1])\n",
    "    pr_auc_v = auc(recall_prauc, precision_prauc) \n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_v,\n",
    "        \"precision\": precision_v,\n",
    "        \"recall\": recall_v,\n",
    "        \"f1\": f1_v, \n",
    "        \"roc_auc\": roc_auc_v, \n",
    "        \"pr_auc\": pr_auc_v\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9b6b60-de0a-4c05-bfd5-e0c3fcb944c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch201/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=data['train'], \n",
    "    eval_dataset=data['test'], \n",
    "    compute_metrics=compute_metrics, \n",
    "    data_collator=data_collator, \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ffa303f-f2c3-44f7-8526-e6eacc9a7e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 10:31, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.394400</td>\n",
       "      <td>0.393392</td>\n",
       "      <td>{'accuracy': 0.8473360655737705}</td>\n",
       "      <td>{'precision': 0.9731182795698925}</td>\n",
       "      <td>{'recall': 0.7225548902195609}</td>\n",
       "      <td>{'f1': 0.8293241695303551}</td>\n",
       "      <td>{'roc_auc': 0.9238585985922891}</td>\n",
       "      <td>0.940020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.243532</td>\n",
       "      <td>{'accuracy': 0.9328893442622951}</td>\n",
       "      <td>{'precision': 0.9421319796954315}</td>\n",
       "      <td>{'recall': 0.9261477045908184}</td>\n",
       "      <td>{'f1': 0.934071464519376}</td>\n",
       "      <td>{'roc_auc': 0.9795577266519592}</td>\n",
       "      <td>0.980482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.397364</td>\n",
       "      <td>{'accuracy': 0.9293032786885246}</td>\n",
       "      <td>{'precision': 0.9}</td>\n",
       "      <td>{'recall': 0.9700598802395209}</td>\n",
       "      <td>{'f1': 0.9337175792507204}</td>\n",
       "      <td>{'roc_auc': 0.9802873200966489}</td>\n",
       "      <td>0.978967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.304286</td>\n",
       "      <td>{'accuracy': 0.9477459016393442}</td>\n",
       "      <td>{'precision': 0.9563894523326572}</td>\n",
       "      <td>{'recall': 0.9411177644710579}</td>\n",
       "      <td>{'f1': 0.9486921529175051}</td>\n",
       "      <td>{'roc_auc': 0.9850105053051791}</td>\n",
       "      <td>0.985793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=460, training_loss=0.13813452442561316, metrics={'train_runtime': 632.5397, 'train_samples_per_second': 11.699, 'train_steps_per_second': 0.727, 'total_flos': 0.0, 'train_loss': 0.13813452442561316, 'epoch': 3.98})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21d962-c525-4d00-a3d4-1c3342f6a985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch201",
   "language": "python",
   "name": "torch201"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
